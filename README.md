# ğŸ‡»ğŸ‡³ Vietnamese Vector Space Model (VSM)

This project builds and visualizes a **Vector Space Model (VSM)** for Vietnamese text using **TF-IDF**, **Word2Vec**, and **FastText**.  
It includes preprocessing, training, similarity search, and t-SNE visualization for both documents and word embeddings.

---

## ğŸ§© Project Structure
<pre>
  Vietnamese-VSM/
  â”œâ”€â”€ data/
  â”‚ â”œâ”€â”€ wikipedia_vi.csv                 # input dataset (id, title, text)
  â”‚ â”œâ”€â”€ stopwords.txt                    # Vietnamese stopword list
  â”‚ â”œâ”€â”€ corpus.txt                       # preprocessed & tokenized text (auto-generated)
  â”‚ â””â”€â”€ meta.csv                         # metadata: id, title (auto-generated)
  â”œâ”€â”€ models/
  â”‚ â”œâ”€â”€ fasttext.bin                     # trained FastText embeddings
  â”‚ â”œâ”€â”€ tfidf_matrix.svd50.npz           # reduced doc matrix (SVD 50)
  â”‚ â”œâ”€â”€ tfidf_vectorizer.joblib          # saved TF-IDF model
  â”‚ â”œâ”€â”€ word2vec.model                   # trained Word2Vec embeddings
  â”‚ â”œâ”€â”€ word2vec.model.syn1neg.npy       # trained Word2Vec embeddings
  â”‚ â””â”€â”€ word2vec.model.wv.vectors.npy    # trained Word2Vec embeddings
  â”œâ”€â”€ make_data.py                       # text preprocessing & corpus generation
  â”œâ”€â”€ using_model_demo.py                # TF-IDF model training & similarity search
  â”œâ”€â”€ visualize.py                       # t-SNE visualization for docs or words
  â”œâ”€â”€ word2vec.py                        # Word2Vec training
  â”œâ”€â”€ fastText.py                        # FastText training
  â””â”€â”€ requirements.txt                   # Python dependencies
</pre>

---

## âš™ï¸ Installation

**_Requires: Python 3.11_**

### (Optional) Create and activate a virtual environment 

```bash
python -m venv venv
source venv/bin/activate          # on macOS/Linux
# or
venv\Scripts\activate             # on Windows
```

### Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ“˜ Data Preparation

Download `wikipedia_vi.`: [here](https://drive.google.com/file/d/1_gFXaM3vFplPnyJGsV1QY5gqgtFjArdg/view?usp=sharing)

Make sure `data/wikipedia_vi.csv` exists before running the program.

Then run:
```bash
python make_data.py --input wikipedia_vi.csv --stopwords stopwords.txt
```
This will generate:
```bash
data/corpus.txt
data/meta.csv
```

---

## ğŸ§  Train and Use TF-IDF Model

Train TF-IDF and save model
```bash
Train TF-IDF and save model
```

Query similar documents
```bash
python using_model_demo.py --query "Äáº¡i há»c Quá»‘c Gia HÃ  Ná»™i"
```



